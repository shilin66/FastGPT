import base64
import os
import time
import asyncio
import io
import json
import traceback
from pathlib import Path
from tempfile import TemporaryDirectory
from typing import Dict, Optional, Annotated, Union, Any

import fitz
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from concurrent.futures import ProcessPoolExecutor
from pydantic import BaseModel, Field, validator
import torch
from contextlib import asynccontextmanager
import torch.multiprocessing as mp
import multiprocessing

from marker.output import text_from_rendered
from marker.converters.pdf import PdfConverter
from marker.converters.table import TableConverter
from marker.converters.ocr import OCRConverter
from marker.converters.extraction import ExtractionConverter
from marker.models import create_model_dict
from marker.config.parser import ConfigParser
from marker.settings import settings

# Configuration parameter model
class ProcessingParams(BaseModel):
    output_format: Annotated[
        str,
        Field(
            description="The format to output the text in. Can be 'markdown', 'json', 'html', or 'chunks'. Defaults to 'markdown'."
        ),
    ] = "markdown"
    page_range: Annotated[
        Optional[str],
        Field(
            description="Page range to convert, specify comma separated page numbers or ranges. Example: 0,5-10,20",
            example=None,
        ),
    ] = None
    force_ocr: Annotated[
        bool,
        Field(
            description="Force OCR on all pages of the PDF. Defaults to False. This can lead to worse results if you have good text in your PDFs."
        ),
    ] = False
    paginate_output: Annotated[
        bool,
        Field(
            description="Whether to paginate the output. Defaults to False. If set to True, each page of the output will be separated by a horizontal rule."
        ),
    ] = False
    use_llm: Annotated[
        bool,
        Field(
            description="Enable LLM processing for enhanced text extraction. Defaults to False."
        ),
    ] = False
    llm_service: Annotated[
        Optional[str],
        Field(
            description="LLM service class to use (e.g., 'marker.services.openai.OpenAIService'). Only used when use_llm is True."
        ),
    ] = None
    format_lines: Annotated[
        bool,
        Field(
            description="Reformat all lines using a local OCR model (inline math, underlines, bold, etc.). This will give very good quality math output."
        ),
    ] = False
    strip_existing_ocr: Annotated[
        bool,
        Field(
            description="Remove all existing OCR text in the document and re-OCR with surya."
        ),
    ] = False
    redo_inline_math: Annotated[
        bool,
        Field(
            description="If you want the absolute highest quality inline math conversion, use this along with use_llm."
        ),
    ] = False
    disable_image_extraction: Annotated[
        bool,
        Field(
            description="Don't extract images from the PDF. If you also specify use_llm, then images will be replaced with a description."
        ),
    ] = False
    converter_cls: Annotated[
        Optional[str],
        Field(
            description="Converter class to use. Options: 'marker.converters.pdf.PdfConverter' (default), 'marker.converters.table.TableConverter', 'marker.converters.ocr.OCRConverter', 'marker.converters.extraction.ExtractionConverter'"
        ),
    ] = None
    force_layout_block: Annotated[
        Optional[str],
        Field(
            description="Force layout detection to assume every page is a specific block type (e.g., 'Table')"
        ),
    ] = None
    keep_chars: Annotated[
        bool,
        Field(
            description="Keep individual characters and bounding boxes (for OCR converter)"
        ),
    ] = False
    block_correction_prompt: Annotated[
        Optional[str],
        Field(
            description="Optional prompt that will be used to correct the output of marker when LLM mode is active"
        ),
    ] = None
    page_schema: Annotated[
        Optional[Dict[str, Any]],
        Field(
            description="JSON schema for structured extraction (for extraction converter)"
        ),
    ] = None
    # LLM service specific parameters
    openai_api_key: Optional[str] = None
    openai_base_url: Optional[str] = None
    openai_model: Optional[str] = None
    gemini_api_key: Optional[str] = None
    vertex_project_id: Optional[str] = None
    ollama_base_url: Optional[str] = None
    ollama_model: Optional[str] = None
    claude_api_key: Optional[str] = None
    claude_model_name: Optional[str] = None
    azure_endpoint: Optional[str] = None
    azure_api_key: Optional[str] = None
    deployment_name: Optional[str] = None
    
    @validator('output_format')
    def validate_output_format(cls, v):
        valid_formats = ['markdown', 'json', 'html', 'chunks']
        if v not in valid_formats:
            raise ValueError(f'output_format must be one of {valid_formats}')
        return v

process_variables = {}
my_pool = None
app_data = {}
app = FastAPI()

def worker_init(counter, lock):
    num_gpus = torch.cuda.device_count()
    processes_per_gpu = int(os.environ.get('PROCESSES_PER_GPU', 1))
    with lock:
        worker_id = counter.value
        counter.value += 1
    if num_gpus == 0:
        device = 'cpu'
        device_id = 0
    else:
        device_id = worker_id // processes_per_gpu
        if device_id >= num_gpus:
            raise ValueError(f"Worker ID {worker_id} exceeds available GPUs ({num_gpus}).")
        device = f'cuda:{device_id}'
    
    # åŸºç¡€é…ç½®å‚æ•°
    config = {
        "output_format": "markdown",
        "pdftext_workers": 1
    }
    
    # åˆå§‹åŒ–è½¬æ¢å™¨ï¼ˆä¸åŒ…å«å…·ä½“å‚æ•°ï¼Œå°†åœ¨å¤„ç†æ—¶åŠ¨æ€é…ç½®ï¼‰
    pid = os.getpid()
    process_variables[pid] = {
        'device_id': device_id,
        'device': device,
        'models': None  # å°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–
    }
    print(f"Worker {worker_id}: Initialized on {device}!")

# æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹çš„åˆå§‹åŒ–å‡½æ•°
def init_converter(params_dict, device_id):
    """æ ¹æ®å‚æ•°åŠ¨æ€åˆå§‹åŒ–è½¬æ¢å™¨"""
    # ç¡®ä¿æ¯ä¸ªè¿›ç¨‹ä½¿ç”¨ç‹¬ç«‹çš„GPUä¸Šä¸‹æ–‡
    if device_id > 0:
        os.environ["CUDA_VISIBLE_DEVICES"] = str(device_id)

    # è°ƒè¯•è¾“å‡º
    print(f"ğŸ”§ Worker {os.getpid()}: åˆå§‹åŒ–è½¬æ¢å™¨ï¼Œå‚æ•°: {params_dict}")
    
    config_parser = ConfigParser(params_dict)
    config_dict = config_parser.generate_config_dict()
    config_dict["pdftext_workers"] = 1
    
    # è°ƒè¯•è¾“å‡ºé…ç½®
    print(f"ğŸ”§ Worker {os.getpid()}: ç”Ÿæˆçš„é…ç½®: use_llm={config_dict.get('use_llm', False)}")
    if config_dict.get('use_llm'):
        print(f"ğŸ”§ Worker {os.getpid()}: LLMæœåŠ¡: {config_dict.get('llm_service', 'None')}")
        llm_service = config_parser.get_llm_service()
        print(f"ğŸ”§ Worker {os.getpid()}: LLMæœåŠ¡å¯¹è±¡: {type(llm_service).__name__ if llm_service else 'None'}")
    
    # å¦‚æœå¯ç”¨äº† LLM ä½†æ²¡æœ‰æŒ‡å®šæœåŠ¡ï¼Œä½¿ç”¨é»˜è®¤çš„ Gemini æœåŠ¡
    if config_dict.get('use_llm') and not config_dict.get('llm_service'):
        config_dict['llm_service'] = 'marker.services.gemini.GoogleGeminiService'
        print(f"ğŸ”§ Worker {os.getpid()}: ä½¿ç”¨é»˜è®¤ LLM æœåŠ¡: Gemini")
    
    # æ ¹æ®converter_clså‚æ•°é€‰æ‹©è½¬æ¢å™¨ç±»å‹
    converter_cls_name = params_dict.get('converter_cls', 'marker.converters.pdf.PdfConverter')
    
    if converter_cls_name == 'marker.converters.table.TableConverter':
        converter_class = TableConverter
    elif converter_cls_name == 'marker.converters.ocr.OCRConverter':
        converter_class = OCRConverter
    elif converter_cls_name == 'marker.converters.extraction.ExtractionConverter':
        converter_class = ExtractionConverter
    else:
        converter_class = PdfConverter
    
    # ä¸ºextraction converterç‰¹æ®Šå¤„ç†
    print(f"ğŸ”§ Worker {os.getpid()}: åˆ›å»ºè½¬æ¢å™¨ï¼Œç±»å‹: {converter_class.__name__}")
    
    try:
        if converter_class == ExtractionConverter:
            converter = converter_class(
                config=config_dict,
                artifact_dict=create_model_dict(),
                llm_service=config_parser.get_llm_service(),
            )
        else:
            converter = converter_class(
                config=config_dict,
                artifact_dict=create_model_dict(),
                processor_list=config_parser.get_processors(),
                renderer=config_parser.get_renderer(),
                llm_service=config_parser.get_llm_service(),
            )
        
        print(f"ğŸ”§ Worker {os.getpid()}: è½¬æ¢å™¨åˆ›å»ºæˆåŠŸ")
        return converter
        
    except Exception as e:
        print(f"âŒ Worker {os.getpid()}: è½¬æ¢å™¨åˆ›å»ºå¤±è´¥: {str(e)}")
        print(f"âŒ Worker {os.getpid()}: é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        raise

def get_converter_for_params(params_dict):
    """ä¸ºç»™å®šå‚æ•°è·å–æˆ–åˆ›å»ºè½¬æ¢å™¨"""
    pid = os.getpid()
    worker_info = process_variables.get(pid)
    
    if not worker_info:
        raise RuntimeError("Worker not properly initialized")
    
    # åˆ›å»ºå‚æ•°çš„å“ˆå¸Œé”®ç”¨äºç¼“å­˜
    param_key = json.dumps(params_dict, sort_keys=True)
    
    if 'converters' not in worker_info:
        worker_info['converters'] = {}
    
    # å¦‚æœå·²æœ‰ç›¸åŒå‚æ•°çš„è½¬æ¢å™¨ï¼Œç›´æ¥è¿”å›
    if param_key in worker_info['converters']:
        return worker_info['converters'][param_key]
    
    # åˆ›å»ºæ–°çš„è½¬æ¢å™¨
    converter = init_converter(params_dict, worker_info['device_id'])
    worker_info['converters'][param_key] = converter
    
    return converter


@asynccontextmanager
async def lifespan(app: FastAPI):
    # ç¡®ä¿è®¾å¤‡è®¾ç½®æ­£ç¡®
    torch_device = os.environ.get('TORCH_DEVICE', 'auto')
    if torch_device == 'auto' or torch_device not in ['cpu', 'cuda', 'mps']:
        if torch.cuda.is_available():
            os.environ['TORCH_DEVICE'] = 'cuda'
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            os.environ['TORCH_DEVICE'] = 'mps'
        else:
            os.environ['TORCH_DEVICE'] = 'cpu'
    
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {os.environ['TORCH_DEVICE']}")
    
    # Initialize shared model dictionary
    app_data["models"] = create_model_dict()
    
    try:
        mp.set_start_method('spawn')
    except RuntimeError:
        raise RuntimeError("Set start method to spawn twice. This may be a temporary issue with the script. Please try running it again.")
    
    global my_pool
    manager = multiprocessing.Manager()
    worker_counter = manager.Value('i', 0)
    worker_lock = manager.Lock()
    gpu_count = torch.cuda.device_count()
    my_pool = ProcessPoolExecutor(
        max_workers=gpu_count*int(os.environ.get('PROCESSES_PER_GPU', 1)), 
        initializer=worker_init, 
        initargs=(worker_counter, worker_lock)
    )
    
    yield
    
    # Cleanup
    if my_pool:
        my_pool.shutdown(wait=True)
    if "models" in app_data:
        del app_data["models"]
    print("Application shutdown, cleaning up...")

app.router.lifespan_context = lifespan

# å­è¿›ç¨‹å¤„ç†å•ä¸ªPDFçš„æ ¸å¿ƒå‡½æ•°
def process_pdf(pdf_path, params_dict, output_dir="./marker2-result"):
    try:
        print(f"ğŸ“„ Worker {os.getpid()}: å¤„ç†PDF {pdf_path}")
        print(f"ğŸ“„ Worker {os.getpid()}: å‚æ•° use_llm={params_dict.get('use_llm', False)}")
        print(f"ğŸ“„ Worker {os.getpid()}: å®Œæ•´å‚æ•°: {params_dict}")
        
        # ç¡®ä¿æœ‰åŸºæœ¬çš„é»˜è®¤å‚æ•°
        if not params_dict:
            params_dict = {
                "output_format": "markdown",
                "use_llm": False,
                "force_ocr": False,
                "format_lines": False
            }
            print(f"ğŸ“„ Worker {os.getpid()}: ä½¿ç”¨é»˜è®¤å‚æ•°: {params_dict}")
        
        # è·å–é€‚åˆå½“å‰å‚æ•°çš„è½¬æ¢å™¨
        print(f"ğŸ“„ Worker {os.getpid()}: å¼€å§‹è·å–è½¬æ¢å™¨...")
        converter = get_converter_for_params(params_dict)
        print(f"ğŸ“„ Worker {os.getpid()}: è½¬æ¢å™¨è·å–æˆåŠŸ: {type(converter).__name__}")
        
        # æ‰§è¡ŒGPUåŠ é€Ÿçš„è½¬æ¢
        print(f"ğŸ“„ Worker {os.getpid()}: å¼€å§‹æ‰§è¡Œè½¬æ¢...")
        rendered = converter(pdf_path)
        print(f"ğŸ“„ Worker {os.getpid()}: è½¬æ¢å®Œæˆï¼Œç»“æœç±»å‹: {type(rendered)}")
        
        # æ ¹æ®è¾“å‡ºæ ¼å¼å¤„ç†ç»“æœ
        output_format = params_dict.get('output_format', 'markdown')
        
        if output_format == 'json':
            # JSONæ ¼å¼ç›´æ¥è¿”å›renderedå¯¹è±¡
            result_data = {
                "status": "success",
                "data": rendered.model_dump() if hasattr(rendered, 'model_dump') else rendered,
                "metadata": rendered.metadata if hasattr(rendered, 'metadata') else {},
                "output_path": output_dir
            }
        elif output_format == 'chunks':
            # Chunksæ ¼å¼
            text, _, images = text_from_rendered(rendered)
            result_data = {
                "status": "success",
                "chunks": rendered.model_dump() if hasattr(rendered, 'model_dump') else rendered,
                "text": text,
                "images": images,
                "metadata": rendered.metadata if hasattr(rendered, 'metadata') else {},
                "output_path": output_dir
            }
        else:
            # Markdown/HTMLæ ¼å¼
            print(f"ğŸ“„ Worker {os.getpid()}: å¼€å§‹æå–æ–‡æœ¬å’Œå›¾åƒ...")
            text, _, images = text_from_rendered(rendered)
            print(f"ğŸ“„ Worker {os.getpid()}: æ–‡æœ¬æå–å®Œæˆï¼Œé•¿åº¦: {len(text) if text else 0}")
            print(f"ğŸ“„ Worker {os.getpid()}: å›¾åƒæ•°é‡: {len(images) if images else 0}")
            
            result_data = {
                "status": "success",
                "text": text,
                "images": images,
                "metadata": rendered.metadata if hasattr(rendered, 'metadata') else {},
                "output_path": output_dir
            }
        
        print(f"ğŸ“„ Worker {os.getpid()}: å‡†å¤‡è¿”å›ç»“æœï¼ŒçŠ¶æ€: {result_data['status']}")
        return result_data
        
    except Exception as e:
        error_msg = str(e)
        error_trace = traceback.format_exc()
        print(f"âŒ Worker {os.getpid()}: å¤„ç†å¼‚å¸¸: {error_msg}")
        print(f"âŒ Worker {os.getpid()}: å¼‚å¸¸è¯¦æƒ…: {error_trace}")
        return {"status": "error", "message": error_msg, "file": pdf_path, "traceback": error_trace}


# --- FastAPIæ¥å£å®ç° ---
async def process_single_file(file: UploadFile, params_dict: Dict, output_dir: str) -> Dict:
    """å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡ä»¶"""
    try:
        global my_pool
        # ä¿å­˜ä¸Šä¼ æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        temp_path = Path(output_dir) / file.filename
        with open(temp_path, "wb") as buffer:
            buffer.write(await file.read())
        
        # è·å–æ–‡æ¡£é¡µæ•°
        pdf_document = fitz.open(temp_path)
        total_pages = pdf_document.page_count
        pdf_document.close()
        
        # æäº¤åˆ°è¿›ç¨‹æ± å¤„ç†
        loop = asyncio.get_running_loop()
        result = await loop.run_in_executor(
            my_pool,
            process_pdf,
            str(temp_path),
            params_dict,
            output_dir
        )

        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        response = {
            "filename": file.filename,
            "status": result["status"],
            "output_path": result.get("output_path", output_dir),
            "pages": total_pages,
        }
        
        if result["status"] == "success":
            response.update({
                "text": result.get("text", ""),
                "images": result.get("images", {}),
                "metadata": result.get("metadata", {}),
                "data": result.get("data"),  # For JSON output
                "chunks": result.get("chunks")  # For chunks output
            })
        else:
            response["error"] = result.get("message", "Unknown error")
            response["traceback"] = result.get("traceback", "")
        
        return response
        
    except Exception as e:
        return {
            "filename": file.filename,
            "status": "error",
            "error": f"å¤„ç†å¼‚å¸¸: {str(e)}",
            "traceback": traceback.format_exc()
        }

def encode_images(images):
    """Encode images to base64 using marker settings"""
    encoded = {}
    for k, v in images.items():
        byte_stream = io.BytesIO()
        v.save(byte_stream, format=settings.OUTPUT_IMAGE_FORMAT)
        encoded[k] = base64.b64encode(byte_stream.getvalue()).decode(
            settings.OUTPUT_ENCODING
        )
    return encoded

def img_to_base64(img_path):
    with open(img_path, "rb") as img_file:
        return base64.b64encode(img_file.read()).decode('utf-8')
def embed_images_as_base64(md_content, images_dict):
    """Embed images as base64 data URIs in markdown content"""
    if not images_dict:
        return md_content
        
    # Encode images to base64
    encoded_images = encode_images(images_dict)
    
    lines = md_content.split('\n')
    new_lines = []
    for line in lines:
        if line.startswith("![") and "](" in line and ")" in line:
            start_idx = line.index("](") + 2
            end_idx = line.index(")", start_idx)
            img_rel_path = line[start_idx:end_idx]

            img_name = os.path.basename(img_rel_path)
            # Remove extension and look for the image in encoded_images
            img_key = os.path.splitext(img_name)[0]
            
            if img_key in encoded_images:
                img_base64 = encoded_images[img_key]
                new_line = f'![](data:image/{settings.OUTPUT_IMAGE_FORMAT.lower()};base64,{img_base64})'
                new_lines.append(new_line)
            else:
                new_lines.append(line)
        else:
            new_lines.append(line)
    return '\n'.join(new_lines)

'''
    curl --location --request POST "http://localhost:7434/v2/parse/file" \
    --header "Authorization: Bearer your_access_token" \
    --form "file=@./file/chinese_test.pdf"
'''
@app.post("/v2/parse/file")
async def process_pdfs(
    file: Optional[UploadFile] = File(default=None),
    files: Optional[list[UploadFile]] = File(default=None),
    # åŸºç¡€å‚æ•°
    output_format: Optional[str] = Form(default=None),
    page_range: Optional[str] = Form(default=None),
    force_ocr: Optional[bool] = Form(default=None),
    paginate_output: Optional[bool] = Form(default=None),
    
    # LLMç›¸å…³å‚æ•°
    use_llm: Optional[bool] = Form(default=None),
    llm_service: Optional[str] = Form(default=None),
    format_lines: Optional[bool] = Form(default=None),
    strip_existing_ocr: Optional[bool] = Form(default=None),
    redo_inline_math: Optional[bool] = Form(default=None),
    disable_image_extraction: Optional[bool] = Form(default=None),
    block_correction_prompt: Optional[str] = Form(default=None),
    
    # è½¬æ¢å™¨ç›¸å…³å‚æ•°
    converter_cls: Optional[str] = Form(default=None),
    force_layout_block: Optional[str] = Form(default=None),
    keep_chars: Optional[bool] = Form(default=None),
    
    # ç»“æ„åŒ–æå–å‚æ•°
    page_schema: Optional[str] = Form(default=None),  # JSONå­—ç¬¦ä¸²
    
    # LLMæœåŠ¡é…ç½®å‚æ•°
    openai_api_key: Optional[str] = Form(default=None),
    openai_base_url: Optional[str] = Form(default=None),
    openai_model: Optional[str] = Form(default=None),
    gemini_api_key: Optional[str] = Form(default=None),
    vertex_project_id: Optional[str] = Form(default=None),
    ollama_base_url: Optional[str] = Form(default=None),
    ollama_model: Optional[str] = Form(default=None),
    claude_api_key: Optional[str] = Form(default=None),
    claude_model_name: Optional[str] = Form(default=None),
    azure_endpoint: Optional[str] = Form(default=None),
    azure_api_key: Optional[str] = Form(default=None),
    deployment_name: Optional[str] = Form(default=None),
    
    # é¢„è®¾é…ç½®
    config_preset: Optional[str] = Form(default=None),  # ä»openai_ocr_config.jsonåŠ è½½é¢„è®¾
    
    # ç‰¹æ®ŠåŠŸèƒ½å‚æ•°
    action: Optional[str] = Form(default=None)  # ç‰¹æ®Šæ“ä½œ: "health", "list_presets", "get_preset"
):
    """å¤„ç†PDFæ–‡ä»¶çš„ç»Ÿä¸€æ¥å£ï¼Œæ”¯æŒæ–‡ä»¶å¤„ç†ã€é…ç½®æŸ¥è¯¢ã€å¥åº·æ£€æŸ¥ç­‰åŠŸèƒ½"""
    s_time = time.time()
    
    print(f"ğŸš€ API: æ”¶åˆ°è¯·æ±‚ï¼Œaction={action}, file={'å­˜åœ¨' if file else 'ä¸å­˜åœ¨'}, files={'å­˜åœ¨' if files else 'ä¸å­˜åœ¨'}")
    
    # å¤„ç†ç‰¹æ®Šæ“ä½œ
    if action == "health":
        # å¥åº·æ£€æŸ¥
        gpu_count = torch.cuda.device_count()
        return {
            "success": True,
            "action": "health",
            "status": "healthy",
            "gpu_count": gpu_count,
            "cuda_available": torch.cuda.is_available(),
            "worker_count": my_pool._max_workers if my_pool else 0,
            "supported_formats": ["markdown", "json", "html", "chunks"],
            "supported_converters": [
                "marker.converters.pdf.PdfConverter",
                "marker.converters.table.TableConverter", 
                "marker.converters.ocr.OCRConverter",
                "marker.converters.extraction.ExtractionConverter"
            ]
        }
    
    elif action == "list_presets":
        # è·å–æ‰€æœ‰é…ç½®é¢„è®¾
        try:
            with open("openai_ocr_config.json", "r", encoding="utf-8") as f:
                presets = json.load(f)
            return {
                "success": True,
                "action": "list_presets",
                "presets": list(presets.keys()),
                "configs": presets
            }
        except FileNotFoundError:
            return {
                "success": False,
                "action": "list_presets",
                "error": "é…ç½®æ–‡ä»¶ openai_ocr_config.json ä¸å­˜åœ¨"
            }
        except Exception as e:
            return {
                "success": False,
                "action": "list_presets",
                "error": f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}"
            }
    
    elif action == "get_preset":
        # è·å–ç‰¹å®šçš„é…ç½®é¢„è®¾
        if not config_preset:
            raise HTTPException(status_code=400, detail="ä½¿ç”¨ get_preset æ“ä½œæ—¶å¿…é¡»æä¾› config_preset å‚æ•°")
        
        try:
            with open("openai_ocr_config.json", "r", encoding="utf-8") as f:
                presets = json.load(f)
            if config_preset in presets:
                return {
                    "success": True,
                    "action": "get_preset",
                    "preset_name": config_preset,
                    "config": presets[config_preset]
                }
            else:
                raise HTTPException(status_code=404, detail=f"é¢„è®¾é…ç½® '{config_preset}' ä¸å­˜åœ¨")
        except FileNotFoundError:
            raise HTTPException(status_code=404, detail="é…ç½®æ–‡ä»¶ openai_ocr_config.json ä¸å­˜åœ¨")
        except HTTPException:
            raise
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    # éªŒè¯æ–‡ä»¶å¤„ç†è¾“å…¥
    if not file and not files:
        raise HTTPException(status_code=400, detail="å¿…é¡»æä¾› file æˆ– files å‚æ•°ï¼Œæˆ–ä½¿ç”¨ action å‚æ•°è¿›è¡Œé…ç½®æŸ¥è¯¢")
    
    if file and files:
        raise HTTPException(status_code=400, detail="ä¸èƒ½åŒæ—¶æä¾› file å’Œ files å‚æ•°")
    
    # ç¡®å®šå¤„ç†æ¨¡å¼
    is_batch = files is not None
    file_list = files if is_batch else [file]
    
    if is_batch and len(file_list) > 10:
        raise HTTPException(status_code=400, detail="æ‰¹é‡å¤„ç†æœ€å¤šæ”¯æŒ10ä¸ªæ–‡ä»¶")
    
    try:
        # æ„å»ºå‚æ•°å­—å…¸ï¼Œè®¾ç½®é»˜è®¤å€¼
        params_dict = {
            "output_format": "markdown",
            "use_llm": False,
            "force_ocr": False,
            "format_lines": False,
            "strip_existing_ocr": False,
            "redo_inline_math": False,
            "disable_image_extraction": False,
            "paginate_output": False,
            "keep_chars": False
        }
        
        # åŠ è½½é¢„è®¾é…ç½®ï¼ˆå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤é¢„è®¾ï¼‰
        default_preset = "åŸºç¡€markdownè½¬æ¢"  # é»˜è®¤é¢„è®¾
        actual_preset = config_preset or default_preset
        
        print(f"ğŸ”§ API: æ¥æ”¶åˆ°çš„ config_preset å‚æ•°: {config_preset}")
        print(f"ğŸ”§ API: å®é™…ä½¿ç”¨çš„é¢„è®¾: {actual_preset}")
        
        try:
            print(f"ğŸ”§ API: å°è¯•è¯»å–é…ç½®æ–‡ä»¶ openai_ocr_config.json...")
            with open("openai_ocr_config.json", "r", encoding="utf-8") as f:
                presets = json.load(f)
            print(f"ğŸ”§ API: é…ç½®æ–‡ä»¶è¯»å–æˆåŠŸï¼ŒåŒ…å« {len(presets)} ä¸ªé¢„è®¾")
            
            if actual_preset in presets:
                preset_config = presets[actual_preset]
                print(f"ğŸ”§ API: æ‰¾åˆ°é¢„è®¾é…ç½®: {preset_config}")
                params_dict.update(preset_config)
                if config_preset:
                    print(f"ğŸ”§ API: ç”¨æˆ·æŒ‡å®šé¢„è®¾é…ç½® '{actual_preset}' åŠ è½½æˆåŠŸ")
                else:
                    print(f"ğŸ”§ API: ä½¿ç”¨é»˜è®¤é¢„è®¾é…ç½® '{actual_preset}' åŠ è½½æˆåŠŸ")
            else:
                print(f"ğŸ”§ API: å¯ç”¨é¢„è®¾: {list(presets.keys())}")
                if config_preset:
                    raise HTTPException(status_code=400, detail=f"é¢„è®¾é…ç½® '{config_preset}' ä¸å­˜åœ¨")
                else:
                    print(f"âš ï¸  API: é»˜è®¤é¢„è®¾ '{default_preset}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€é»˜è®¤å‚æ•°")
                    
        except FileNotFoundError:
            print(f"ğŸ”§ API: é…ç½®æ–‡ä»¶ openai_ocr_config.json ä¸å­˜åœ¨")
            if config_preset:
                raise HTTPException(status_code=400, detail="é…ç½®æ–‡ä»¶ openai_ocr_config.json ä¸å­˜åœ¨")
            else:
                print(f"âš ï¸  API: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€é»˜è®¤å‚æ•°")
        except Exception as e:
            print(f"âŒ API: è¯»å–é…ç½®æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            print(f"âŒ API: å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
            if config_preset:
                raise HTTPException(status_code=500, detail=f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            else:
                print(f"âš ï¸  API: é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€é»˜è®¤å‚æ•°")
        
        # ç”¨è¡¨å•å‚æ•°è¦†ç›–é¢„è®¾é…ç½®
        # æ„å»ºè¡¨å•å‚æ•°å­—å…¸ï¼ŒåªåŒ…å«éNoneçš„å€¼
        form_params = {}
        
        # å­—ç¬¦ä¸²å‚æ•° - åªæœ‰éNoneæ—¶æ‰æ·»åŠ 
        if output_format is not None:
            form_params["output_format"] = output_format
        if page_range is not None:
            form_params["page_range"] = page_range
        if llm_service is not None:
            form_params["llm_service"] = llm_service
        if block_correction_prompt is not None:
            form_params["block_correction_prompt"] = block_correction_prompt
        if converter_cls is not None:
            form_params["converter_cls"] = converter_cls
        if force_layout_block is not None:
            form_params["force_layout_block"] = force_layout_block
        
        # LLMæœåŠ¡é…ç½®å‚æ•° - åªæœ‰éNoneæ—¶æ‰æ·»åŠ 
        if openai_api_key is not None:
            form_params["openai_api_key"] = openai_api_key
        if openai_base_url is not None:
            form_params["openai_base_url"] = openai_base_url
        if openai_model is not None:
            form_params["openai_model"] = openai_model
        if gemini_api_key is not None:
            form_params["gemini_api_key"] = gemini_api_key
        if vertex_project_id is not None:
            form_params["vertex_project_id"] = vertex_project_id
        if ollama_base_url is not None:
            form_params["ollama_base_url"] = ollama_base_url
        if ollama_model is not None:
            form_params["ollama_model"] = ollama_model
        if claude_api_key is not None:
            form_params["claude_api_key"] = claude_api_key
        if claude_model_name is not None:
            form_params["claude_model_name"] = claude_model_name
        if azure_endpoint is not None:
            form_params["azure_endpoint"] = azure_endpoint
        if azure_api_key is not None:
            form_params["azure_api_key"] = azure_api_key
        if deployment_name is not None:
            form_params["deployment_name"] = deployment_name
        
        # å¸ƒå°”å‚æ•° - åªæœ‰æ˜¾å¼ä¼ é€’Trueæ—¶æ‰è¦†ç›–é¢„è®¾
        # è¿™æ ·å¯ä»¥é¿å…é»˜è®¤çš„Falseå€¼è¦†ç›–é¢„è®¾é…ç½®ä¸­çš„Trueå€¼
        if force_ocr is True:
            form_params["force_ocr"] = force_ocr
        if paginate_output is True:
            form_params["paginate_output"] = paginate_output
        if use_llm is True:
            form_params["use_llm"] = use_llm
        if format_lines is True:
            form_params["format_lines"] = format_lines
        if strip_existing_ocr is True:
            form_params["strip_existing_ocr"] = strip_existing_ocr
        if redo_inline_math is True:
            form_params["redo_inline_math"] = redo_inline_math
        if disable_image_extraction is True:
            form_params["disable_image_extraction"] = disable_image_extraction
        if keep_chars is True:
            form_params["keep_chars"] = keep_chars
        
        # æ·»åŠ è°ƒè¯•è¾“å‡º
        print(f"ğŸ”§ API: é¢„è®¾é…ç½®åŠ è½½åçš„å‚æ•°: {params_dict}")
        print(f"ğŸ”§ API: æ˜¾å¼ä¼ é€’çš„è¡¨å•å‚æ•°: {form_params}")
        
        # ç”¨è¡¨å•å‚æ•°æ›´æ–°é¢„è®¾é…ç½®
        params_dict.update(form_params)
        
        print(f"ğŸ”§ API: æœ€ç»ˆå‚æ•°å­—å…¸: {params_dict}")
        
        # å¤„ç†page_schema JSONå­—ç¬¦ä¸²
        if page_schema:
            try:
                params_dict["page_schema"] = json.loads(page_schema)
            except json.JSONDecodeError:
                raise HTTPException(status_code=400, detail="page_schema å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONå­—ç¬¦ä¸²")
        
        # éªŒè¯è¾“å‡ºæ ¼å¼
        valid_formats = ['markdown', 'json', 'html', 'chunks']
        if params_dict.get('output_format', 'markdown') not in valid_formats:
            raise HTTPException(status_code=400, detail=f"output_format å¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€: {valid_formats}")
        
        # å¤„ç†æ–‡ä»¶ï¼ˆå•æ–‡ä»¶æˆ–æ‰¹é‡ï¼‰
        if is_batch:
            # æ‰¹é‡å¤„ç†
            results = []
            total_processing_time = 0
            
            for current_file in file_list:
                file_start_time = time.time()
                try:
                    with TemporaryDirectory() as temp_dir:
                        temp_path = Path(temp_dir) / current_file.filename
                        with open(temp_path, "wb") as buffer:
                            buffer.write(await current_file.read())
                        
                        # è·å–æ–‡æ¡£é¡µæ•°
                        pdf_document = fitz.open(temp_path)
                        total_pages = pdf_document.page_count
                        pdf_document.close()
                        
                        # æäº¤åˆ°è¿›ç¨‹æ± å¤„ç†
                        loop = asyncio.get_running_loop()
                        file_result = await loop.run_in_executor(
                            my_pool,
                            process_pdf,
                            str(temp_path),
                            params_dict,
                            temp_dir
                        )
                        
                        file_processing_time = time.time() - file_start_time
                        total_processing_time += file_processing_time
                        
                        if file_result.get("status") == "success":
                            # æ„å»ºå•æ–‡ä»¶å“åº”
                            file_response = {
                                "filename": current_file.filename,
                                "success": True,
                                "pages": total_pages,
                                "processing_time": file_processing_time,
                                "output_format": params_dict.get('output_format', 'markdown')
                            }
                            
                            output_format = params_dict.get('output_format', 'markdown')
                            
                            if output_format == 'json':
                                file_response["data"] = file_result.get("data")
                                file_response["metadata"] = file_result.get("metadata", {})
                            elif output_format == 'chunks':
                                file_response["chunks"] = file_result.get("chunks")
                                file_response["metadata"] = file_result.get("metadata", {})
                                if file_result.get("text"):
                                    file_response["text"] = file_result["text"]
                            else:
                                # markdown æˆ– html æ ¼å¼
                                text = file_result.get("text", "")
                                images = file_result.get("images", {})
                                
                                if output_format == 'markdown' and images:
                                    # åµŒå…¥base64å›¾åƒ
                                    text = embed_images_as_base64(text, images)
                                
                                file_response["text"] = text
                                file_response["markdown"] = text  # ä¿æŒå‘åå…¼å®¹
                                file_response["images"] = images
                                file_response["metadata"] = file_result.get("metadata", {})
                            
                            results.append(file_response)
                        else:
                            results.append({
                                "filename": current_file.filename,
                                "success": False,
                                "error": file_result.get("message", "Unknown error"),
                                "traceback": file_result.get("traceback", ""),
                                "processing_time": file_processing_time
                            })
                            
                except Exception as e:
                    results.append({
                        "filename": current_file.filename,
                        "success": False,
                        "error": f"å¤„ç†å¼‚å¸¸: {str(e)}",
                        "traceback": traceback.format_exc(),
                        "processing_time": time.time() - file_start_time
                    })
            
            # è¿”å›æ‰¹é‡å¤„ç†ç»“æœ
            return {
                "success": True,
                "message": "",
                "batch": True,
                "total_files": len(file_list),
                "total_processing_time": total_processing_time,
                "results": results
            }
        
        else:
            # å•æ–‡ä»¶å¤„ç†
            with TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir) / file.filename
                with open(temp_path, "wb") as buffer:
                    buffer.write(await file.read())
                
                # è·å–æ–‡æ¡£é¡µæ•°
                pdf_document = fitz.open(temp_path)
                total_pages = pdf_document.page_count
                pdf_document.close()
                
                # æäº¤åˆ°è¿›ç¨‹æ± å¤„ç†
                loop = asyncio.get_running_loop()
                results = await loop.run_in_executor(
                    my_pool,
                    process_pdf,
                    str(temp_path),
                    params_dict,
                    temp_dir
                )
                
                processing_time = time.time() - s_time
                
                if results.get("status") != "success":
                    return {
                        "success": False,
                        "message": "",
                        "error": results.get("message", "Unknown error"),
                        "traceback": results.get("traceback", ""),
                        "processing_time": processing_time
                    }
                
                # æ ¹æ®è¾“å‡ºæ ¼å¼æ„å»ºå“åº”
                response = {
                    "success": True,
                    "message": "",
                    "batch": False,
                    "pages": total_pages,
                    "processing_time": processing_time,
                    "output_format": params_dict.get('output_format', 'markdown')
                }
                
                output_format = params_dict.get('output_format', 'markdown')
                
                if output_format == 'json':
                    response["data"] = results.get("data")
                    response["metadata"] = results.get("metadata", {})
                elif output_format == 'chunks':
                    response["chunks"] = results.get("chunks")
                    response["metadata"] = results.get("metadata", {})
                    if results.get("text"):
                        response["text"] = results["text"]
                else:
                    # markdown æˆ– html æ ¼å¼
                    text = results.get("text", "")
                    images = results.get("images", {})
                    
                    if output_format == 'markdown' and images:
                        # åµŒå…¥base64å›¾åƒ
                        text = embed_images_as_base64(text, images)
                    
                    response["text"] = text
                    response["markdown"] = text  # ä¿æŒå‘åå…¼å®¹
                    response["images"] = images
                    response["metadata"] = results.get("metadata", {})
                
                return response
            
    except HTTPException:
        raise
    except Exception as e:
        return {
            "success": False,
            "message": "",
            "error": f"å¤„ç†å¼‚å¸¸: {str(e)}",
            "traceback": traceback.format_exc(),
            "processing_time": time.time() - s_time
        }






if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=7434)
